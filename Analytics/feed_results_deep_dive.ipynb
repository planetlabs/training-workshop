{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"ps-header\">\n",
    "    <a href=\"https://developers.planet.com/\" target=\"_blank\">\n",
    "<img src=\"https://www.planet.com/assets/logos/logo.svg\" style=\"display:inline-block; margin-right: 10px;\" width=\"100\"/> <span style=\"font-size:18px\">Developer Resource Center</span>\n",
    "</a>\n",
    "    \n",
    "###  Planet Analytics API Tutorial\n",
    "\n",
    "<h1 style=\"margin-top:10px;\">Planet Analytic Feeds Results</h1>\n",
    "</div>\n",
    "<div class=\"content-block\">\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. [Introduction](#1.-Introduction) \n",
    "\n",
    "> Introduction to Analytic Feed Results types: Object Detection vs Segmentation\n",
    "\n",
    "2. [Getting Results](#1.-Getting-Results) \n",
    "\n",
    "> Get Results for a Subscription from the API\n",
    "\n",
    "3. [Working with Planet Analytics Results](#3.-Working-with-Planet-Analytics-Results) \n",
    "\n",
    "> Interpret and visualize Results with source satellite imagery\n",
    "\n",
    "4. [Segmentation Results](#4.-Segmentation-Results)\n",
    "\n",
    "> Visualizing Segementation masks\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In the previous notebook, we explored the Planet Analytics Feeds API mechanics and core concepts of **Feeds** and **Subscriptions**. We worked through making requests to the api, understanding the response data, and showed how to create some basic visualizations with the data. It's encouraged to work through the first notebook before continuing.\n",
    "\n",
    "In this notebook, we'll focus on Planet Analytic Feeds **Results**. Results represent the \"detections\" made in source Planet satellite imagery scenes for different types of objects visible on the ground. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytic Feed Results\n",
    "\n",
    "**Results** on the Planet Analytics API represent the output or \"detections\" of our machine learning models. Results are created for each Subscription, and each Subscription is derived from a Feed:\n",
    "\n",
    "*Feed → Subscription → Results* \n",
    "\n",
    "> When new Planet imagery is published that intersects a Subscription's AOI and TOI, Planet’s computer vision models process the imagery and the output is added to a \"collection\" (WFS 3) of Results associated with a Subscription.\n",
    "\n",
    "\n",
    "#### Feed / Result Types\n",
    "\n",
    "As we've seen, several types of **Feeds** exist, and Results for Feed Subscriptions can be categorized as one of two types: `Object Detection` and `Segmentation`.\n",
    "\n",
    "<div class=\"callout-blue top-margin\">\n",
    "\n",
    "#### Types of Feeds + Result Output Format\n",
    "\n",
    "| Feed Type | Results Type | Results Format | Results Access |\n",
    "| --- | --- | --- | --- |\n",
    "| Vessel Detection | Object Detection | Detection Features (Polygons) | WFS 3 (Detection Features) |\n",
    "| Building Detection | Segmentation (Classification) | Raster Mask / Basemaps | WFS 3 (Quads) / XYZ Tiles |\n",
    "| Road Detection | Segmentation (Classification) | Raster Mask / Basemaps | WFS 3 (Quads) / XYZ Tiles |\n",
    "\n",
    "</div>\n",
    "\n",
    "Outputs for **all** Results (of any Feed type) are delivered via `WFS 3` compliant endpoint on the API. The \"shape\" of the data representing Results for each Subscription are specific to the Subscription's Feed. \n",
    "\n",
    "*Object Detection** type Feeds Results output are returned as `GeoJSON Features` with polygonal geometries representing bounding boxes around detections made by the machine learning model. For example, a box around a ship. \n",
    "\n",
    "*Segmentation* type Feeds Results output are `GeoJSON Features` that describe \"Quads\" (individual raster files) that make up a larger segmentation / classification \"mask\". The Planet Analytics API lists links to these segmentation quads on the Planet Mosaics API where the raster files are stored, but also links to XYZ basemap tiles that can be used to view the raster data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WFS 3\n",
    "\n",
    "\n",
    "<img src=\"http://www.opengeospatial.org/pub/www/files/OGC_Logo_2D_Blue_x_0_0.png\" align=\"right\" style=\"margin-left: 20px\" width=\"100\"></img>\n",
    "\n",
    "The Planet Analytics API's **Results** (`collections` and `items`) endpoints follow the [Open Geospatial Consortium's](http://www.opengeospatial.org) (OGC) [Web Feature Service 3.0](https://github.com/opengeospatial/WFS_FES) (WFS) specification.\n",
    "\n",
    "\n",
    "> A Web Feature Service (WFS) is a standard API that represents collections of geospatial data.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting Results\n",
    "\n",
    "Let's get started by accessing one of our Subscriptions. We'll see how we can get it's associated **Results** and we'll explore and visualize the data!\n",
    "\n",
    "First we'll setup our API `base url` and `authentication` so we can make our `request`, just like in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configure Auth and Base Url\n",
    "\n",
    "# Planet Analytics API base url\n",
    "PAA_BASE_URL = \"https://api.planet.com/analytics/\"\n",
    "\n",
    "# API Key Config\n",
    "API_KEY = os.environ['PL_API_KEY']\n",
    "\n",
    "# Setup Auth\n",
    "BASIC_AUTH = (API_KEY, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a couple helper functions for \"pretty printing\" our responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty Print Helper\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# JSON Pretty Print Helper\n",
    "import json\n",
    "\n",
    "def jpp(data):\n",
    "    print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the `Requests` library to get the Subscription. We'll get the Subscription using it's `id`, which is a [\"Universally Unique Identifier](https://en.wikipedia.org/wiki/Universally_unique_identifier) (UUID) string. \n",
    "\n",
    "*Remember, we can see a list of available Subscriptions (and their `ids` by making a request to the `/subscriptions` endpoint. See the previous tutorial for more details.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request a Subscription\n",
    "\n",
    "import requests\n",
    "\n",
    "# Define the Subscripton UUID\n",
    "subscription_id = \"9db92275-1d89-4d3b-a0b6-68abd2e94142\"\n",
    "\n",
    "# Construct the url for the Subscription\n",
    "subscription_url = PAA_BASE_URL + 'subscriptions/' + subscription_id\n",
    "\n",
    "print(\"Request URL: {}\".format(subscription_url))\n",
    "\n",
    "# Make the GET request for Subscriptions list \n",
    "subscription = requests.get(subscription_url, auth=BASIC_AUTH).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the Subscription we just requested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some data\n",
    "print(\"{} \\n{}\\nSubscription Id: {}\\n\".format(subscription['title'], subscription['description'], subscription['id']))\n",
    "\n",
    "# Print the subscription object\n",
    "print(json.dumps(subscription, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output should look familiar from the previous tutorial. We see the title, AOI, TOI, and other metadata about the Subscription.\n",
    "\n",
    "Now we can move on to getting the **Results** for this Subscription:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subscription Results Collection\n",
    "\n",
    "We can use the Subscription `id` property to get the **Results** `collection` and `items` for this Subscription.\n",
    "\n",
    "We can get the `collection` by making a request to the `/collections` endpoint with the `id` from our Subscription:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the url for the subscription's Results collection\n",
    "subscription_results_url = PAA_BASE_URL + 'collections/' + subscription['id']\n",
    "\n",
    "print(\"Request URL: {}\".format(subscription_results_url))\n",
    "\n",
    "# Get subscription results collection\n",
    "subscription_results = requests.get(subscription_results_url, auth=BASIC_AUTH).json()\n",
    "\n",
    "# Pretty Print reponse json\n",
    "print(json.dumps(subscription_results, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subscription Results Items\n",
    "\n",
    "Now let's get the actual **Results** `items` from the `collection`. Our request endpoint should be constructed in like this: `/collections/{ID}/items`\n",
    "\n",
    "But we don't need to construct the **Results** `items` url manually, it's one of the `links` that's provided in our Subscription response. Let's use the link from our Subscription response from the previous cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the url\n",
    "subscription_items_url = list(filter(lambda link: link['rel'] == 'results', subscription['links']))[0]['href']\n",
    "\n",
    "print(\"Subscription Items URL:\\n{}\\n\".format(subscription_items_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `url` to make our request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request subscription Items\n",
    "\n",
    "# Get request Subscription features (collection items)\n",
    "subscription_items = requests.get(subscription_items_url, auth=BASIC_AUTH).json()\n",
    "\n",
    "# Pretty Print reponse json\n",
    "print(\"Subscription Results Collection Items: (GeoJSON FeatureCollection):\\n {}\".format(json.dumps(subscription_items, sort_keys=True, indent=4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now we've got our first set of **Results** data. The **Results** for a Subscription come to us as a `GeoJSON` `FeatureCollection`.\n",
    "\n",
    "Each `item` is a `GeoJSON Feature` listed in the the `features` property of the `FeatureCollection`. Let's check how many `items` (Features) we have so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of Features in the subscription's collection:\n",
    "print(\"\\nNumber of Items (Features): {} \\n\".format(len(subscription_items['features'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The request we made to get these `items` is actually a **paged** list of results. \n",
    "\n",
    "In our `GeoJSON` Feature Collection, we see there is a `links` property, which conforms to the [WFS 3.0 Spec](https://github.com/opengeospatial/WFS_FES/blob/master/core/openapi/schemas/featureCollectionGeoJSON.yaml#L14) in accordance with the [OpenAPI spec for Links](https://swagger.io/docs/specification/links/). \n",
    "\n",
    "Let's look at the `links` property to see how we can get all the **Results** `items` for our Subscription:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty Print the links\n",
    "pp.pprint(subscription_items['links'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `\"rel\" : \"next\"` property's `href` should contain a URL with a [`query string`](https://en.wikipedia.org/wiki/Query_string) `?before={ITEM_ID}` where the `ITEM_ID` is the UUID identifier.\n",
    "\n",
    "We can use the `link` with the `rel` value \"`next`\" to get the next page of data, and follow each subsequent `next` link to request all the pages of data and get the entire `FeatureCollection` for this subscription's items. \n",
    "\n",
    "Let's write a helper function to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to get the \"next\" link\n",
    "def get_next_link(links):\n",
    "    for link in links:\n",
    "        if link['rel'] == 'next':\n",
    "            return link['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our helper function to get the first \"next\" link:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first \"next\" link\n",
    "next_link =  get_next_link(subscription_items['links']) \n",
    "\n",
    "print(next_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's iterate through our data and follow each \"next\" link to get all the data associated with our Subscription's Resuts Collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_number = 2\n",
    "# Loop to get all pages of features\n",
    "while next_link:\n",
    "    print(\"Getting page number {} at {}\".format(page_number, next_link))\n",
    "    \n",
    "    # Use \"next\" link to request next page\n",
    "    next_collection = requests.get(next_link, auth=BASIC_AUTH).json()\n",
    "        \n",
    "    print(\"Found {} additional Features \\n\".format(len(next_collection['features'])))\n",
    "    \n",
    "    # Get the next \"next\" link\n",
    "    next_link = get_next_link(next_collection[\"links\"])\n",
    "\n",
    "    # Add features from new page to our original list of features\n",
    "    subscription_items['features'].extend(next_collection['features'])\n",
    "    \n",
    "    # increment the page number\n",
    "    page_number = page_number + 1\n",
    "    \n",
    "print(\"Total Subscription Items: {}\".format(len(subscription_items['features'])))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also increase the number of results returned (default 250, max 10000) \n",
    "by changing the limit query parameter on the number of results that are returned:\n",
    "\n",
    "`requests.get(subscription_items_url, params={\"limit\": 500} )`\n",
    "\n",
    "Here's our final **Results** `FeatureCollection`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have our complete GeoJSON Feature Collection\n",
    "subscription_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Working with Planet Analytics Results\n",
    "\n",
    "Now that we've got our **Results** let's use use GeoPandas to explore our data like we did in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Make a GeoPandas data frame from our collection items\n",
    "# Since we're importing a GeoJSON file, we can use the `from_features` method\n",
    "gdf = gpd.GeoDataFrame.from_features(subscription_items['features'])\n",
    "\n",
    "# Show the first five results from our GeoDataFrame\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we created the DataFrame with the `.from_features()` method, GeoPandas automatically converted our `GeoJSON` geometries to `Shapely` shapes.\n",
    "\n",
    "### Result Item Properties\n",
    "\n",
    "Let's take a look at the rest of the columns available to us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the item properties (columns)\n",
    "for column in gdf.columns.values:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these properties should be self-explanatory, and different Feed types may include different properties.\n",
    "\n",
    "##### Source Properties\n",
    "The `source_` properties all relate to the source satellite imagery that the detections were made against. We'll see how we can use these later on.\n",
    "\n",
    "##### Detection Object Properties\n",
    "The `object_` properties describe some meta-data about each detection, like length and diagonal. \n",
    "\n",
    "Also relevant are `measure`, `score` (the score assigned by the machine learning model), `geometry` (the \"footprint\" of the detection), and `observed`.\n",
    "\n",
    "Let's convert our `observed` column data to a `datetime`. The `observed` column represents the date that a detection was made, based on when the source satellite image was taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert observed column into a datetime datatype\n",
    "gdf[\"observed\"] = pd.to_datetime(gdf[\"observed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our **Results**. For starters, we can create a simple plot using GeoPandas built in `plot` method. Later on we'll make some more intersting interactive visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple GeoPandas Plot of features\n",
    "gdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting a single detection\n",
    "We can use our GeoPandas GeoDataFrame to look at the first detection item. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first detection by index\n",
    "first_detection = gdf.iloc[0]\n",
    "first_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the first detection's geometry visually. When we created our GeoDataFrame from our collection items data, the features were automatically converted to Shapely Geometries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use shapely to show the first geometry\n",
    "first_detection['geometry']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Results with GeoViews\n",
    "\n",
    "That's simple enough, but doesn't really give us much context. Let's use [GeoViews](http://geo.holoviews.org) to make some more interesting interactive visualizations!\n",
    "\n",
    "\n",
    "<div class=\"callout-light top-margin\">\n",
    "    \n",
    "<img src=\"http://geo.holoviews.org/_static/geoviews-logo.png\" align=\"right\" style=\"margin-left: 20px\" width=\"100\"></img>\n",
    "\n",
    "\n",
    "GeoViews is built on [HoloViews](http://holoviews.org/) and uses [Cartopy](http://scitools.org.uk/cartopy), [matplotlib](http://matplotlib.org/) and [Bokeh](http://bokeh.pydata.org/) for visualizations. It is released as part of the [PyViz](http://pyviz.org/) suite.\n",
    "\n",
    "> GeoViews is a Python library that makes it easy to explore and visualize geographical, meteorological, and oceanographic datasets, such as those used in weather, climate, and remote sensing research.\n",
    "\n",
    "</div>\n",
    "\n",
    "We can import GeoViews and it's parent library HoloViews like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import GeoViews\n",
    "import geoviews as gv\n",
    "\n",
    "# Import HoloViews\n",
    "import holoviews as hv\n",
    "from holoviews import opts as hvOpts\n",
    "\n",
    "# Set rendering backends\n",
    "gv.extension('bokeh')\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a basemap to use in our visualizations\n",
    "basemap = gv.tile_sources.CartoLight\n",
    "\n",
    "# List built in GeoViews tile sources \n",
    "# help(gv.tile_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the detection geometry on a basemap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the detection using GeoViews Shape element with a basemap\n",
    "gv.Shape(first_detection['geometry']).opts(padding=0.5, width=500, height=400) * basemap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! GeoViews natively handles our Shapely Geometries and has no issue plotting our detection using an interative map with Bokeh. \n",
    "\n",
    "If we zoom out a bit, we can start to get some more context by seeing where the detection occurred on the map.\n",
    "\n",
    "**In the next section, we'll take a look at combining Planet Analytics Data with Planet Imagery**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytics Detections and  Source Imagery\n",
    "\n",
    "The metadata associated with a single detection contains some interesting information, but of course we're here because we want to see the detection with our own eyes too! \n",
    "\n",
    "We can easily access the satellite imagery data associated with each detection via several of Planet's [Data API](https://developers.planet.com/docs/api/) and [Tile services](https://developers.planet.com/docs/api/tile-services/), and we'll be able to view `metadata`, and `imagery previews` if we like, as well as download the imagery `assets`. \n",
    "\n",
    "In order to find the image, we'll need the `scene id` and [`item_type`](https://developers.planet.com/docs/api/items-assets/) (sensor type) for the satellite image used to observe the detection.\n",
    "\n",
    "In our analytics detections data, sensor type is stored as the `source_item_type` property, and scene id is stored as the `source_item_id`. \n",
    "\n",
    "**Let's get this information for a single detection from our subscription data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sensor type\n",
    "item_type = first_detection['source_item_type']\n",
    "\n",
    "# Get the item id\n",
    "scene_id = first_detection['source_item_id']\n",
    "\n",
    "print(\"Item Type: {} \\nScene Id: {}\".format(item_type, scene_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting observation source imagery data\n",
    "\n",
    "Using just these two peices of information, we can access everything we need to know about the satellite imagery used in the observation by getting it from Planet's Data API! \n",
    "\n",
    "We'll use the [Planet Python Client](https://planetlabs.github.io/planet-client-python/) in this example, but you can also make a request to the API directly with your preferred approach by constructing a url that includes the `item_id` and `item_type`:\n",
    "\n",
    "`https://api.planet.com/data/v1/item-types/{ITEM TYPE}/items/{ITEM ID}`\n",
    "\n",
    "For more in depth guides on working with the Planet Data API, visit our [Planet School](https://developers.planet.com/planetschool/) section on the Planet [Developer Resource Center](https://developers.planet.com) website.\n",
    "\n",
    "We can get information on our source satellite imagery scene by using the `get_item` method from the Planet Python Client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Planet Data Api module\n",
    "from planet import api as PlanetAPI\n",
    "\n",
    "# Create a client for the API\n",
    "planet_client = PlanetAPI.ClientV1(api_key=API_KEY)\n",
    "\n",
    "# Get data for our item\n",
    "source_scene = planet_client.get_item(item_type, scene_id).get()\n",
    "\n",
    "# Pretty Print our scene info\n",
    "jpp(source_scene)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result should be a `GeoJSON Feature` with the following noteable fields: \n",
    "\n",
    "* The scene metadata in `properties`\n",
    "* The scene footprint in `geometry`\n",
    "* Links to the scene imagery assets and previews in `_links` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source Imagery Previews and Visualizations\n",
    "\n",
    "Let's actually view the scene viually by using the [`thumbnail`](https://developers.planet.com/docs/api/item-previews) link to preview our scene imagery with IPython:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# Get the thumbnail url\n",
    "# Append our api key to authenticate\n",
    "thumb_url = source_scene['_links']['thumbnail'] + '?api_key=' + API_KEY\n",
    "\n",
    "# Render the preview image with IPython.display\n",
    "Image(url=thumb_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a nice preview of our image, so we can take a peek without having to download the entire asset!\n",
    "\n",
    "But we won't need to get all the scene meta data available on the Planet Data Api just to see the preview. We only really need the `item_type` and `scene_id` to construct urls for the scene image preview thumbnail, as well as (`xyz`) [tiles](https://developers.planet.com/docs/api/tile-services) we can use in a web map.\n",
    "\n",
    "The urls to these imagery previews look like:\n",
    "\n",
    "* Thumbnails: `https://tiles.planet.com/data/v1/item-types/{ITEM TYPE}/items/{ITEM ID}/thumb`\n",
    "* XYZ Tiles: `https://tiles.planet.com/data/v1/{ITEM TYPE}/{ITEM ID}/{Z}/{X}/{Y}.png`\n",
    "\n",
    "Let's write a helper function that will let us grab the source imagery that our detections were observed in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get source imagery helper utility\n",
    "# Takes type xyz or thumb\n",
    "def get_source_imagery_url(scene_id, scene_sensor, preview_type=\"xyz\"):\n",
    "    \n",
    "    # Determine parts of url\n",
    "    prefix = \"\" if preview_type == \"xyz\" else \"item-types/\"\n",
    "    middle = \"/\" if preview_type == \"xyz\" else \"/items/\" \n",
    "    suffix = \"/{Z}/{X}/{Y}.png\" if preview_type == \"xyz\" else \"/thumb\"\n",
    "    \n",
    "    # Construct the url\n",
    "    source_imagery_url = \"https://tiles.planet.com/data/v1/\" + prefix + scene_sensor + middle + scene_id + suffix + \"?api_key=\" + API_KEY\n",
    "    return source_imagery_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our helper function, we can construct the preview urls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "# Get the scene thumbnail url\n",
    "thumb_url = get_source_imagery_url(scene_id, item_type, 'thumb')\n",
    "\n",
    "# Get the scene tiles url\n",
    "tiles_url = get_source_imagery_url(scene_id, item_type, 'xyz')\n",
    "\n",
    "# Print the tile url\n",
    "print(\"XYZ Web Map Tiles Url:\\n{}\\n\".format(tiles_url))\n",
    "\n",
    "# Print the thumbnail link\n",
    "print(\"Thumbnail URL:\")\n",
    "HTML('<a href=\"{}\" target=\"_blank\">Thumbnail Url</a>'.format(thumb_url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View a single detection with it's source Planet Imagery\n",
    "\n",
    "Let's put our preview images to work by overlaying a detection on a web map with the observation imagery xyz tiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the detection geometry\n",
    "single_detection = gv.Shape(first_detection['geometry'], label=\"Detection in {} - {}\".format(item_type, scene_id)).opts(\n",
    "    fill_alpha=0, \n",
    "    line_color=\"red\", \n",
    "    width=600, height=500, padding=3,\n",
    ")\n",
    "\n",
    "# Plot with web map tiles\n",
    "single_detection * basemap * gv.WMTS(data=tiles_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an interactive map with our detection geometry and Planet source imagery. Pan around the map to see more of the scene!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize All Detections \n",
    "\n",
    "#### Detection Thumbnails\n",
    "\n",
    "Let's use this same method to create thumbnails to view more detections with their source imagery. Let's create a function to render these visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to render a thumbnail for an item\n",
    "# Takes an index (row) from DataFrame\n",
    "def render_item_thumb(index):\n",
    "    # Get the item row from DataFrame\n",
    "    item = gdf.iloc[index]\n",
    "    \n",
    "    # Construct tile url\n",
    "    tile_url = get_source_imagery_url(item['source_item_id'], item['source_item_type'], 'xyz')   \n",
    "    \n",
    "    # Create detection plot\n",
    "    detection = gv.Shape(item['geometry']).opts(\n",
    "        fill_alpha=0, \n",
    "        line_color=\"red\", \n",
    "        width=200, height=200,\n",
    "        default_tools=[],\n",
    "        xaxis=None,\n",
    "        yaxis=None\n",
    "    ) \n",
    "    \n",
    "    return (detection * basemap * gv.WMTS(data=tile_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use our helper function to render a Holoviews `Layout`. We'll render the first twenty detections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the first 20 thumbs\n",
    "thumbs_list = [render_item_thumb(i) for i in list(gdf.index[0:20])]\n",
    "\n",
    "# Create a Layout visualization\n",
    "layout = hv.Layout(thumbs_list).opts(shared_axes=False).cols(4)\n",
    "layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactive Detections Plot\n",
    "\n",
    "Using GeoViews, we can also plot all our detections on a single interactive map and color our detections based on their `score` values. \n",
    "\n",
    "We can include a histogram using the `.hist` method on our GeoViews `Polygons` element to show a break down of overall scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all features on a map\n",
    "\n",
    "# Pick the properties (dimensions) we want to include\n",
    "items_dimensions = ['score', 'object_diagonal_m', 'observed']\n",
    "\n",
    "# Plot the features with GeoViews\n",
    "all_features_plot = gv.Polygons(gdf, vdims=items_dimensions).opts(\n",
    "    colorbar_position=\"bottom\", colorbar_opts={\"title\": \"score\"},\n",
    "    cmap=\"RdYlBu_r\", \n",
    "    width=600, height=500,\n",
    "    colorbar=True,\n",
    "    title=\"All Detections for {}\".format(subscription['title']))\n",
    "\n",
    "# Plot the features map with a histogram and basemap \n",
    "all_features_plot.hist() * basemap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can transform our polygons into points using the `.to` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points = all_features_plot.to(gv.Points, vdims=['score'])\n",
    "all_points.options(width=800, height=600) * basemap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Large Datasets\n",
    "We can even handle millions of data points using the [Datashader](http://holoviews.org/user_guide/Large_Data.html) operation from HoloViews. Here we'll render our features as points (centroid of polygons) using Datashader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holoviews.operation.datashader import datashade\n",
    "from colorcet import fire\n",
    "\n",
    "(datashade(all_points, cmap=fire, width=800, height=600) * basemap).options(width=800, height=500, title=\"Datashaded Points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize All Detections with Planet Imagery\n",
    "\n",
    "Now lets combine these plots with our source satellite imagery, as we did earlier for a single detection.\n",
    "\n",
    "It will be useful to know the overall \"bounds\" of our data. We can get this with GeoPandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the plot bounds\n",
    "bounds = tuple(gdf.total_bounds)\n",
    "bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Subscription's **Results** collection may have hundreds or thousands of detection items... Let's use a Holoviews [`DynamicMap`](http://holoviews.org/reference/containers/matplotlib/DynamicMap.html) to create an interactive webmap that will let us click on any detection data point and render the imagery in an adjacent plot. This will be great for reducing the visual clutter that would result in rendering all the detections' observation scenes at the same time.\n",
    "\n",
    "The code in the next cell uses some more advanced `HoloViews` concepts like [`Select1D streams`](http://holoviews.org/reference/streams/bokeh/Selection1D.html) and [`Ndoverlay`](http://holoviews.org/reference/containers/bokeh/NdOverlay.html), and DynamicMaps to make an interactive visualization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Detection Viewer using GeoViews DynamicMap\n",
    "\n",
    "from holoviews.streams import Selection1D\n",
    "from shapely.geometry import box, Polygon\n",
    "\n",
    "# Master plot\n",
    "masterplot = all_features_plot.options(\n",
    "    fill_alpha=0, nonselection_line_alpha=0.1, line_color=\"score\",  selection_line_color=\"green\",\n",
    "    tools=[\"hover\", \"tap\", \"save\", \"reset\"], toolbar=\"above\")\n",
    "\n",
    "# Define our stream (responds to Tap event from our master plot)\n",
    "selection_stream = Selection1D(source=masterplot, transient=True)\n",
    "    \n",
    "# Callback Function for DynamicMap to render selection detail    \n",
    "def render_selection(index):\n",
    "    \n",
    "    # Selection Size\n",
    "    selection_plot_size = 300\n",
    "    \n",
    "    # Data Row Index\n",
    "    # We only use the FIRST selection\n",
    "    row_index = index[0] if index else 0\n",
    "    \n",
    "    # Get the detection data   \n",
    "    detection_data = gdf.iloc[row_index]\n",
    "    \n",
    "    # Set the label to the detection id if we have a selection\n",
    "    label = \"Dectection: {}\".format(row_index) if index else 'No Selection' \n",
    "    \n",
    "    # Define the detection geometry (or use total bounds if no selection)\n",
    "    detection_geometry =  detection_data['geometry'] if index else box(*bounds)\n",
    "    \n",
    "    # Get the Extent\n",
    "    detail_extent = detection_geometry.bounds if index else bounds\n",
    "    \n",
    "    # Define the detection Shape\n",
    "    detection_shape = gv.Shape(detection_geometry).opts(\n",
    "        fill_alpha=0, line_alpha=0, \n",
    "        show_bounds=True, \n",
    "        toolbar=\"below\",\n",
    "        width=selection_plot_size, \n",
    "        height=selection_plot_size,\n",
    "        default_tools=[\"pan\", \"wheel_zoom\", \"save\",  \"reset\"])\n",
    "    \n",
    "    # Dict to hold our imagery layers\n",
    "    imagery_layers = {\n",
    "        # Define the basemap\n",
    "        \"basemap\": gv.WMTS(basemap.data).opts(default_tools=[], width=selection_plot_size)\n",
    "    }\n",
    "    \n",
    "    # Table data\n",
    "    table_data = gdf.loc[[*index], ['score', 'source_item_type', 'source_item_id']]\n",
    "    table_data['row'] = index\n",
    "    \n",
    "    # Create a Table with score data\n",
    "    table = hv.Table(table_data, label=\"Selected Detections Properties (First is plotted)\").opts(width=selection_plot_size)\n",
    "    \n",
    "    # Check if we have a selection\n",
    "    if index:\n",
    "                \n",
    "        #Get the detection's imagery type and id\n",
    "        detail_scene_id = detection_data['source_item_id']\n",
    "        detail_scene_sensor = detection_data['source_item_type']\n",
    "\n",
    "        # Get the imagery tile url\n",
    "        image_tile_url = get_source_imagery_url(detail_scene_id, detail_scene_sensor, 'xyz')\n",
    "        \n",
    "        # Create the imagery tile layer\n",
    "        detail_imagery = gv.WMTS(image_tile_url).opts(\n",
    "            width=selection_plot_size, \n",
    "            height=selection_plot_size)\n",
    "        \n",
    "        # Add the imagery layer to our tile layers\n",
    "        imagery_layers[\"imagery\"] = detail_imagery\n",
    "        \n",
    "        # Style the detections box\n",
    "        detection_shape.opts(fill_alpha=0, line_alpha=1, line_color=\"red\")\n",
    "\n",
    "    else:\n",
    "        # Clear the imagery layer if nothing is selected\n",
    "        if \"imagery\" in imagery_layers:\n",
    "            imagery_layers.pop('imagery', None)\n",
    "        \n",
    "    # Create tile layers\n",
    "    tiles = hv.NdOverlay(imagery_layers)\n",
    "        \n",
    "    # Create the detail layout\n",
    "    detail_layout =  (detection_shape * tiles).opts(\n",
    "        title=label, \n",
    "        fontsize={\"title\": \"8pt\"},\n",
    "        xaxis=None, yaxis=None, \n",
    "        width=selection_plot_size, \n",
    "        height=selection_plot_size\n",
    "    ) + table\n",
    "        \n",
    "    return detail_layout.cols(1)\n",
    "\n",
    "# Create the selection plot using a DynamicMap\n",
    "selection_plot = hv.DynamicMap(render_selection, streams=[selection_stream]).collate()\n",
    "\n",
    "# Create the layout and set options\n",
    "layout = (masterplot * basemap  + selection_plot).options(\n",
    "    shared_axes=False, merge_tools=False, \n",
    "    fontsize={\"title\": \"14pt\"},\n",
    "    title=\"Interactive Detections and Imagery\")\n",
    "\n",
    "# Show the layout\n",
    "layout.cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize by Time Dimension\n",
    "\n",
    "We can also make a visualization that let's us interactively view detections for certain days using a HoloMap. First let's organize our data by day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket the data by time interval\n",
    "\n",
    "# Set the interval\n",
    "interval = \"D\" # W = Week, D = Day, Month = M\n",
    "\n",
    "time_bucketed_data = gdf.set_index('observed')[[\n",
    "    \"source_item_id\", \n",
    "    \"geometry\", \n",
    "    \"source_cloud_cover\", \n",
    "    \"score\"]].groupby([pd.Grouper(freq=interval)]).agg({\n",
    "    'geometry': 'count', \n",
    "    'source_item_id': 'nunique', \n",
    "    'source_cloud_cover': \"mean\", \n",
    "    'score':'mean'\n",
    "}).fillna(0).rename(index=str, columns={\"geometry\": \"detection_count\", \"source_item_id\": \"imagery_count\", \"source_cloud_cover\":\"mean_cloud_cover\", \"score\":\"mean_score\"})\n",
    "\n",
    "# Remove rows without detections\n",
    "time_bucketed_data = time_bucketed_data[time_bucketed_data[\"detection_count\"] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's define the render function to use for each day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "# Render Features for a date\n",
    "def plot_features_for_date(start_date):\n",
    "    \n",
    "    # Increment start date by one day\n",
    "    end_date = start_date + timedelta(days=1)\n",
    "\n",
    "    # Slice our data to only include features between start and end date\n",
    "    mask = (gdf[\"observed\"] > start_date) & (gdf[\"observed\"] <= end_date)\n",
    "    sliced_data = gdf.loc[mask]\n",
    "    \n",
    "    # Create the Polygons Plot\n",
    "    return gv.Polygons(sliced_data, vdims=['score']).opts(fill_alpha=0, nonselection_line_alpha=0.1, line_color=\"score\",  selection_line_color=\"green\", cmap=\"RdYlBu_r\", width=600, height=400, tools=[\"hover\", \"tap\"], title=\"Detections for {} on {}\".format(subscription['title'], start_date))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's create our plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render HoloViews HoloMap for \n",
    "\n",
    "# Define the date dimension\n",
    "kdims = [hv.Dimension(('start_date', 'Observation Date'))]\n",
    "\n",
    "# Create all the plots\n",
    "features_for_date = {(date): plot_features_for_date(date) for date in list(pd.to_datetime(time_bucketed_data.index))}\n",
    "\n",
    "# Create the HoloMap \n",
    "features_by_day_plot = hv.HoloMap(features_for_date, kdims=kdims)\n",
    "\n",
    "# Add a basemap to the features plot and render\n",
    "features_by_day_plot * basemap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Segmentation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've looked at an **Object Detection** Feed's Results. Let's switch over to a **Segmentation Feed** and see how we can work with the raster mask data. \n",
    "\n",
    "Since the output data for a Segmentation Feed is comprised of `mosaics` for each interval that we run our model, we can use the mosaics to visualize and explore our data.\n",
    "\n",
    "We'll use the Subscription Information, and subsequently the Feed information to determine where the Raster Result basemaps are available using the [Planet Mosaics API](https://developers.planet.com/docs/api/reference/#tag/Basemaps-and-Mosaics). Later we'll see how we can get the actual raster data as well.\n",
    "\n",
    "First let's get a new Subscription that uses a *Segmentation* Feed type using a Subscription Id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the url for a Segmentation Feed Subscription \n",
    "seg_subscription_url = PAA_BASE_URL + 'subscriptions/' + 'e0c33581-5080-4e81-98a1-2d555e91b41e'\n",
    "\n",
    "print(\"Request URL: {}\".format(subscription_url))\n",
    "\n",
    "# Make the GET request for Subscription\n",
    "seg_subscription = requests.get(seg_subscription_url, auth=BASIC_AUTH).json()\n",
    "\n",
    "seg_subscription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `links` property to get the link to the Subscription's **Feed** and then request the Feed from the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the Feed request url from the Subscription link\n",
    "seg_subscription_feed_url = list(filter(lambda link: link['rel'] == 'feed', seg_subscription['links']))[0]['href']\n",
    "print(seg_subscription_feed_url)\n",
    "\n",
    "# Make the request for the Subscription's Feed\n",
    "seg_subscription_feed = requests.get(seg_subscription_feed_url, auth=BASIC_AUTH).json()\n",
    "seg_subscription_feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our **Feed** data for the Subscription, let's use the `target` property to determine our `mosaic series`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mosaics series id\n",
    "mosaic_target_series_id = seg_subscription_feed['target']['config']['series_id']\n",
    "mosaic_target_series_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Mosaic Series Id, we can make a request to the Planet Mosaics API using the following endpoint `https://api.planet.com/basemaps/v1/series/{series_id}`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the mosaics series url\n",
    "mosaic_target_series_url = \"https://api.planet.com/basemaps/v1/series/\" + mosaic_target_series_id\n",
    "print(mosaic_target_series_url)\n",
    "\n",
    "# Make a request to the Mosaics API\n",
    "target_mosaic_series = requests.get(mosaic_target_series_url, auth=BASIC_AUTH).json()\n",
    "target_mosaic_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some metadata here about our series, including the `first_acquired` and `last_acquired` dates, the `interval` at which a new mosaic gets created. \n",
    "\n",
    "Now that we have the Mosaic series, we can see that there's a link to `mosaics` under the `_links` property. These are the individual raster segmentation mask \"mosaics\" made up of several smaller image files called \"quads\". The mosaics themselves are also available as `xyz` tile basemaps, served by Planet's tile servers. Let's make a final request to the `mosaics` link which will give us the urls we need to make a visualization of the Subscription's resuts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a request to get the subscription's mosaics\n",
    "subscription_mosaics = requests.get(target_mosaic_series['_links']['mosaics'], auth=BASIC_AUTH).json()\n",
    "\n",
    "# Pretty print results\n",
    "jpp(subscription_mosaics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response from our last request includes a `mosaics` property that contains an array/list of individual mosaics available for the series. \n",
    "\n",
    "Among the properties available for each `mosaic`, there is the familiar `_links` property with a `tiles` link that provides us an `xyz` tile url that we can use in a web map.\n",
    "\n",
    "Let's make a new pandas DataFrame with our mosaics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# Create a dataframe from list of mosaics (normalize json to break out individual links)\n",
    "mosaics_df = pd.DataFrame(json_normalize(subscription_mosaics['mosaics']))\n",
    "mosaics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the two peices of information that are most useful for us here, the `tiles` links and the `last_acquired` date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with limited columns\n",
    "target_tiles_df = mosaics_df.loc[:, ['_links.tiles', 'last_acquired']]\n",
    "\n",
    "# Rename the tiles column\n",
    "target_tiles_df.rename(columns={'_links.tiles':'target_tileurl', 'last_acquired':'date'}, inplace=True)\n",
    "\n",
    "# Only use year/month for date\n",
    "target_tiles_df['date'] = pd.to_datetime(target_tiles_df[\"date\"])\n",
    "target_tiles_df['date'] = target_tiles_df['date'].dt.strftime('%Y/%m')\n",
    "\n",
    "target_tiles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a small function to help us format our tile urls for HoloViews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to format target tile urls for holoviews\n",
    "def format_target_mosaic_url(in_url):\n",
    "    url = in_url.replace(\"{x}\", \"{X}\").replace(\"{y}\", \"{Y}\").replace(\"{z}\", \"{Z}\")\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make an interactive visualization of each interval's tiles using the a HoloMap. We'll also add a `query parameter` to the `target` tiles url that allow us to style the target tiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import shape\n",
    "\n",
    "# Get the extent of our subscription\n",
    "seg_subscription_bounds = shape(seg_subscription['geometry']).bounds\n",
    "\n",
    "# Style the target mosaic\n",
    "target_style = \"&exp=bincat:0|a50f15\"\n",
    "\n",
    "# Create data for HoloMap\n",
    "mosaics = {(mosaic['date']):gv.WMTS(format_target_mosaic_url(mosaic['target_tileurl']) + target_style, extents=seg_subscription_bounds) for index, mosaic in target_tiles_df.iterrows() }\n",
    "\n",
    "# Create the visualization\n",
    "subscription_mosaics_plot = hv.HoloMap(mosaics, kdims = [hv.Dimension(('date', 'Date'))]).opts(width=600, height=500)\n",
    "basemap * subscription_mosaics_plot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also able to show the source imagery that the Subscription output was detected against! Remember how we got the Subscription's `target` mosaic series? We can do the same for the `source` mosaic series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the source mosaics series id\n",
    "mosaic_source_series_id = seg_subscription_feed['source']['config']['series_id']\n",
    "print(\"Source Mosaic Series Id: {}\\n\".format(mosaic_source_series_id))\n",
    "\n",
    "# Construct the source mosaics series url\n",
    "mosaic_source_series_url = \"https://api.planet.com/basemaps/v1/series/\" + mosaic_source_series_id\n",
    "print(\"Source Mosaic Series Url: {}\\n\".format(mosaic_source_series_url))\n",
    "\n",
    "# Make a request to the Mosaics API for source mosaics series\n",
    "source_mosaic_series = requests.get(mosaic_source_series_url, auth=BASIC_AUTH).json()\n",
    "\n",
    "# Make a request to the Mosaics API for source mosaics\n",
    "source_mosaics = requests.get(source_mosaic_series['_links']['mosaics'], auth=BASIC_AUTH).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's prepare our Source Mosaic data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a dataframe from list of mosaics (normalize json to break out individual links)\n",
    "src_mosaics_df = pd.DataFrame(json_normalize(source_mosaics['mosaics']))\n",
    "\n",
    "# Create a new DataFrame with limited columns\n",
    "src_tiles_df = src_mosaics_df.loc[:, ['_links.tiles', 'last_acquired']]\n",
    "\n",
    "# Rename the tiles column\n",
    "src_tiles_df.rename(columns={'_links.tiles':'src_tileurl', 'last_acquired':'date'}, inplace=True)\n",
    "\n",
    "# Only use year/month for date\n",
    "src_tiles_df['date'] = pd.to_datetime(src_tiles_df[\"date\"])\n",
    "src_tiles_df['date'] = src_tiles_df['date'].dt.strftime('%Y/%m')\n",
    "\n",
    "src_tiles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge the two tile url DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge src and target tile urls\n",
    "comined_tiles = pd.merge(target_tiles_df, src_tiles_df, on='date')\n",
    "comined_tiles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create the visualization with both source and target imagery:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data for HoloMap\n",
    "mosaics = {(mosaic['date']): gv.WMTS(format_target_mosaic_url(mosaic['target_tileurl']) + target_style, extents=seg_subscription_bounds) + gv.WMTS(format_target_mosaic_url(mosaic['src_tileurl'])) for index, mosaic in comined_tiles.iterrows() }\n",
    "\n",
    "# Create the visualization\n",
    "combined_mosaics_plot = hv.HoloMap(mosaics, kdims = [hv.Dimension(('date', 'Date'))]).opts(width=600, height=500).collate()\n",
    "basemap * combined_mosaics_plot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above visualization, you're able to compare the source imagery with the Subscription's output!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Segmentation Mask Data\n",
    "\n",
    "So far we've seen how to visualize segmentation mask data using Planet's Basemap services which serve up web map tiles. We're also able to retreive the \"raw\" raster files that make up these basemaps via the Planet Mosaics API as well as the Planet Analytics API **Results** for the Subscription. The raster files are available as [Cloud Optimized GeoTiff](https://www.cogeo.org/)(COG) \"quads\" that make up a mosaic.\n",
    "\n",
    "For a Subscription with a *segmentation Feed* type, the `/collection/{ID}/items` endpoint will provide a listing of all mosaic quads processed by our Subscription's Feed*:\n",
    "\n",
    "\\* *Remember, the Results `.../items/` endpoint is paged. In this example, we won't get all the paged items.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_subscription_results_url = PAA_BASE_URL + 'collections/' + seg_subscription['id'] + '/items'\n",
    "\n",
    "print(\"Request URL: {}\".format(seg_subscription_results_url))\n",
    "\n",
    "# Make the GET request for Subscription Results\n",
    "seg_subscription_results = requests.get(seg_subscription_results_url, auth=BASIC_AUTH).json()\n",
    "\n",
    "seg_subscription_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `Feature` in the response `FeatureCollection` represents a quad, and each Feature's `geometry` is the `footprint` of that quad. \n",
    "\n",
    "To illustrate this, let's make a GeoDataFrame from the **Results** `FeatureCollection` for our *segmentation type* Subscription, and plot the quads along with the boundary of our subscription AOI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GeoDataFrame of quads\n",
    "quads_gdf = gpd.GeoDataFrame.from_features(seg_subscription_results['features'])\n",
    "\n",
    "# Plot the quads\n",
    "quads_plot = gv.Polygons(quads_gdf, label=\"Quads\").opts(fill_alpha=0.1, fill_color=\"yellow\", show_legend=True)\n",
    "\n",
    "# Plot the subscription AOI\n",
    "subscription_aoi = gv.Shape(shape(seg_subscription['geometry']), label=\"AOI\").opts(fill_alpha=0.5, line_color=\"red\",  fill_color=\"red\", show_legend=True)\n",
    "\n",
    "# Plot our visualization\n",
    "(quads_plot * subscription_aoi * basemap).opts(width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the geometries of the quads all intersect with our Subscription AOI. \n",
    "\n",
    "#### Downloading a Result Quad\n",
    "\n",
    "If we want to download the raw quad data, for either the source or output target, we can do some via the Planet Mosaics API. To find the link to the file, we can look at the **Result** item's `links` property . Here's the first result from our Subscription Results collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_sub_results_links = seg_subscription_results['features'][0]['links']\n",
    "seg_sub_results_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the links for both `target-quad`(the result output), and `source-quad` (the source imagery that was used to create the detections):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the url to source quad\n",
    "source_quad = list(filter(lambda link: link['rel'] == 'source-quad', seg_sub_results_links))[0]['href']\n",
    "\n",
    "print(\"Source Quad URL:\\n{}\\n\".format(source_quad))\n",
    "\n",
    "# Construct the url to target quad\n",
    "target_quad = list(filter(lambda link: link['rel'] == 'target-quad', seg_sub_results_links))[0]['href']\n",
    "\n",
    "print(\"Target (Result) Quad URL:\\n{}\\n\".format(target_quad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clicking the links in the above cell will download the COG (`.tiff`) file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "**Congratulations!** You've now seen how to download, interpret, and visualize Planet Analytics API **Results**! We've only just scratched the surface of visualizing and exploring these datasets in these tutorials, but this introduction to the Planet Analytics API should enable you to begin building your own applications on top of the API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "#notebook-container {\n",
    "    font-family: 'Gotham SSm A','Gotham SSm B','Helvetica Neue',Helvetica,sans-serif; \n",
    "}\n",
    "#notebook-container a {\n",
    "    color:#009da5; \n",
    "}\n",
    "\n",
    "#notebook-container h1,\n",
    "#notebook-container h2,\n",
    "#notebook-container h3,\n",
    "#notebook-container h4,\n",
    "#notebook-container h5 {\n",
    "    color: #006266;\n",
    "    \n",
    "}\n",
    "\n",
    "#ps-header {\n",
    "    color: white; \n",
    "    padding: 20px; \n",
    "    position:relative;\n",
    "    z-index:2;\n",
    "    background: linear-gradient(to bottom, rgba(0,0,0,0.65) 0%,rgba(0,0,0,0) 100%);\n",
    "        background-color:red !important;\n",
    "}\n",
    "#ps-header:after {\n",
    "    content: '';\n",
    "    position: absolute;\n",
    "    top:0;\n",
    "    left:0;\n",
    "    right:0;\n",
    "    bottom:0;\n",
    "    z-index: -1;\n",
    "    background-image:url(\"https://developers.planet.com/theme/images/lake-okeechobee.jpg\");\n",
    "    background-color:black;\n",
    "    background-size:cover;\n",
    "}\n",
    "\n",
    "#ps-header a {\n",
    "    color:white;\n",
    "    text-decoration:none; \n",
    "}\n",
    "\n",
    "#ps-header h1 {\n",
    "    color: white;\n",
    "}\n",
    "#ps-header h3 {\n",
    "    color:#009da5; \n",
    "        font-weight:normal;\n",
    "}\n",
    "\n",
    ".content-block {\n",
    "    background-color:#f5f5f5; \n",
    "    padding: 10px 20px;\n",
    "} \n",
    "\n",
    ".callout {\n",
    "    border: 2px solid #4f5258;\n",
    "    padding:  10px 20px;\n",
    "}\n",
    ".callout-light {\n",
    "    border: 2px solid #f5f5f5; \n",
    "    padding: 10px 20px;\n",
    "}\n",
    "\n",
    ".callout-blue {\n",
    "    border: 2px solid #009da5; \n",
    "    padding:  10px 20px;\n",
    "    \n",
    "}\n",
    ".callout-filled {\n",
    "    padding:  10px 20px;\n",
    "    \n",
    "}\n",
    "\n",
    ".callout-red {\n",
    "    border: 2px solid #bf3b3b; \n",
    "    padding:  10px 20px;\n",
    "}\n",
    "\n",
    ".success {\n",
    "    color: #2ab26a;\n",
    "}\n",
    "\n",
    ".top-margin {\n",
    "    margin-top: 20px;\n",
    "}\n",
    "</style>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
